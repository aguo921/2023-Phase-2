{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Training Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and connect to workplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "# Load the workspace from the saved config file\n",
    "ws = Workspace.from_config()\n",
    "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a parameterised training script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Folder Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "\n",
    "# Create a folder for the experiment files\n",
    "training_folder = 'training'\n",
    "os.makedirs(training_folder, exist_ok=True)\n",
    "\n",
    "# Copy the data file into the experiment folder\n",
    "shutil.copy(\n",
    "    '../1. Analysis and Preprocessing/preprocessed_datasets/market_segmentation_interaction.csv',\n",
    "    os.path.join(training_folder, \"market_segmentation_interaction.csv\")\n",
    ")\n",
    "shutil.copy(\n",
    "    '../1. Analysis and Preprocessing/preprocessed_datasets/market_segmentation.csv',\n",
    "    os.path.join(training_folder, \"market_segmentation.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $training_folder/lg-training.py\n",
    "# Import libraries\n",
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "import argparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# Set regularization hyperparameter\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--reg_rate', type=float, dest='reg', default=0.01)\n",
    "args = parser.parse_args()\n",
    "reg = args.reg\n",
    "\n",
    "# load the market segmentation dataset\n",
    "print(\"Loading Data...\")\n",
    "market_segmentation = pd.read_csv('market_segmentation_interaction.csv')\n",
    "\n",
    "# Separate features and labels\n",
    "X, y = market_segmentation.drop(columns=\"Segmentation\"), market_segmentation.Segmentation\n",
    "\n",
    "# Split data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "\n",
    "# Train a logistic regression model\n",
    "print('Training a logistic regression model with regularization rate of', reg)\n",
    "run.log('Regularization Rate',  np.float(reg))\n",
    "model = LogisticRegression(C=1/reg).fit(X_train, y_train)\n",
    "\n",
    "# calculate accuracy\n",
    "y_hat = model.predict(X_test)\n",
    "acc = np.average(y_hat == y_test)\n",
    "print('Accuracy:', acc)\n",
    "run.log('Accuracy', np.float(acc))\n",
    "\n",
    "# calculate AUC\n",
    "label_binarizer = LabelBinarizer().fit(y_train)\n",
    "y_onehot_test = label_binarizer.transform(y_test)\n",
    "y_scores = model.predict_proba(X_test)\n",
    "for class_of_interest in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "    class_id = np.flatnonzero(label_binarizer.classes_ == class_of_interest)[0]\n",
    "    auc = roc_auc_score(y_onehot_test[:,class_id],y_scores[:,class_id])\n",
    "    print(f'AUC {class_of_interest} vs rest: ' + str(auc))\n",
    "    run.log(f'AUC {class_of_interest} vs rest', np.float(auc))\n",
    "\n",
    "# Save the trained model in the outputs folder\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "joblib.dump(value=model, filename='outputs/lg-model.pkl')\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $training_folder/rf-training.py\n",
    "# Import libraries\n",
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "import argparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# Set hyperparameters\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--max_features', type=int, dest='max_features', default=4)\n",
    "parser.add_argument('--max_depth', type=int, dest='max_depth', default=None)\n",
    "parser.add_argument('--min_samples_split', type=int, dest='min_samples_split', default=2)\n",
    "parser.add_argument('--min_samples_leaf', type=int, dest='min_samples_leaf', default=1)\n",
    "args = parser.parse_args()\n",
    "max_features = args.max_features\n",
    "max_depth = args.max_depth\n",
    "min_samples_split = args.min_samples_split\n",
    "min_samples_leaf = args.min_samples_leaf\n",
    "\n",
    "# load the market segmentation dataset\n",
    "print(\"Loading Data...\")\n",
    "market_segmentation = pd.read_csv('market_segmentation.csv')\n",
    "\n",
    "# Separate features and labels\n",
    "X, y = market_segmentation.drop(columns=\"Segmentation\"), market_segmentation.Segmentation\n",
    "\n",
    "# Split data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "\n",
    "# Train a random forest model\n",
    "print(\n",
    "    'Training a random forest model with max features of', max_features,\n",
    "    'max depth of', max_depth,\n",
    "    'min samples split of', min_samples_split,\n",
    "    'and min_samples_leaf of', min_samples_leaf\n",
    ")\n",
    "run.log('Maximum depth of tree', max_depth)\n",
    "run.log('Maximum features per split', max_features)\n",
    "run.log('Minimum samples requierd for split', min_samples_split)\n",
    "run.log('Minimum samples per leaf', min_samples_leaf)\n",
    "model = RandomForestClassifier(\n",
    "    max_features=max_features,\n",
    "    max_depth=max_depth,\n",
    "    min_samples_split=min_samples_split,\n",
    "    min_samples_leaf=min_samples_leaf\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "# calculate accuracy\n",
    "y_hat = model.predict(X_test)\n",
    "acc = np.average(y_hat == y_test)\n",
    "print('Accuracy:', acc)\n",
    "run.log('Accuracy', np.float(acc))\n",
    "\n",
    "# calculate AUC\n",
    "label_binarizer = LabelBinarizer().fit(y_train)\n",
    "y_onehot_test = label_binarizer.transform(y_test)\n",
    "y_scores = model.predict_proba(X_test)\n",
    "for class_of_interest in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "    class_id = np.flatnonzero(label_binarizer.classes_ == class_of_interest)[0]\n",
    "    auc = roc_auc_score(y_onehot_test[:,class_id],y_scores[:,class_id])\n",
    "    print(f'AUC {class_of_interest} vs rest: ' + str(auc))\n",
    "    run.log(f'AUC {class_of_interest} vs rest', np.float(auc))\n",
    "\n",
    "# Save the trained model in the outputs folder\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "joblib.dump(value=model, filename='outputs/rf-model.pkl')\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a compute cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "cluster_name = \"aguo921\"\n",
    "\n",
    "try:\n",
    "    # Check for existing compute target\n",
    "    training_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # If it doesn't already exist, create it\n",
    "    try:\n",
    "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2)\n",
    "        training_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "        training_cluster.wait_for_completion(show_output=True)\n",
    "    except Exception as ex:\n",
    "        print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $training_folder/environment.yml\n",
    "name: batch_environment\n",
    "dependencies:\n",
    "- python=3.6.2\n",
    "- scikit-learn\n",
    "- pandas\n",
    "- numpy\n",
    "- pip\n",
    "- pip:\n",
    "  - azureml-defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment, ScriptRunConfig, Environment\n",
    "from azureml.core.runconfig import DockerConfiguration\n",
    "\n",
    "# Create a Python environment for the experiment (from a .yml file)\n",
    "env = Environment.from_conda_specification(\"experiment_env\", training_folder + \"/environment.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run the script with arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a script config for training the logistic regression model\n",
    "script_config = ScriptRunConfig(\n",
    "    source_directory=training_folder,\n",
    "    script='lg-training.py',\n",
    "    arguments = ['--reg_rate', 0.1],\n",
    "    environment=env,\n",
    "    docker_runtime_config=DockerConfiguration(use_docker=True),\n",
    "    compute_target=cluster_name\n",
    ") \n",
    "\n",
    "# submit the experiment run for training the logistic regression model\n",
    "experiment_name = 'train-segmentation-lg'\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)\n",
    "run_lg = experiment.submit(config=script_config)\n",
    "\n",
    "# Block until the experiment run has completed\n",
    "run_lg.wait_for_completion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get logged metrics and files for logistic regression training\n",
    "metrics = run_lg.get_metrics()\n",
    "for key in metrics.keys():\n",
    "    print(key, \":\", metrics.get(key))\n",
    "print('\\n')\n",
    "for file in run_lg.get_file_names():\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a script config for training the random forest model\n",
    "script_config = ScriptRunConfig(\n",
    "    source_directory=training_folder,\n",
    "    script='rf-training.py',\n",
    "    arguments = ['--max_depth', 50, '--max_features', 5, '--min_samples_split', 4, '--min_samples_leaf', 4],\n",
    "    environment=env,\n",
    "    docker_runtime_config=DockerConfiguration(use_docker=True),\n",
    "    compute_target=cluster_name\n",
    ") \n",
    "\n",
    "# submit the experiment run for training the random forest model\n",
    "experiment_name = 'train-segmentation-rf'\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)\n",
    "run_rf = experiment.submit(config=script_config)\n",
    "\n",
    "# Block until the experiment run has completed\n",
    "run_rf.wait_for_completion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get logged metrics and files for random forest training\n",
    "metrics = run_rf.get_metrics()\n",
    "for key in metrics.keys():\n",
    "    print(key, \":\", metrics.get(key))\n",
    "print('\\n')\n",
    "for file in run_rf.get_file_names():\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Register the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the logistic regression model\n",
    "run_lg.register_model(\n",
    "    model_path='outputs/lg-model.pkl', model_name='segmentation-lg',\n",
    "    tags={'Training context':'Script'},\n",
    "    properties={\n",
    "        'AUC A vs rest': run_lg.get_metrics()['AUC A vs rest'],\n",
    "        'AUC B vs rest': run_lg.get_metrics()['AUC B vs rest'],\n",
    "        'AUC C vs rest': run_lg.get_metrics()['AUC C vs rest'],\n",
    "        'AUC D vs rest': run_lg.get_metrics()['AUC D vs rest'],\n",
    "        'Accuracy': run_lg.get_metrics()['Accuracy']\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the random forest model\n",
    "run_rf.register_model(\n",
    "    model_path='outputs/rf-model.pkl', model_name='segmentation-rf',\n",
    "    tags={'Training context':'Script'},\n",
    "    properties={\n",
    "        'AUC A vs rest': run_rf.get_metrics()['AUC A vs rest'],\n",
    "        'AUC B vs rest': run_rf.get_metrics()['AUC B vs rest'],\n",
    "        'AUC C vs rest': run_rf.get_metrics()['AUC C vs rest'],\n",
    "        'AUC D vs rest': run_rf.get_metrics()['AUC D vs rest'],\n",
    "        'Accuracy': run_rf.get_metrics()['Accuracy']\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List registered models\n",
    "for model in Model.list(ws):\n",
    "    print(model.name, 'version:', model.version)\n",
    "    for tag_name in model.tags:\n",
    "        tag = model.tags[tag_name]\n",
    "        print ('\\t',tag_name, ':', tag)\n",
    "    for prop_name in model.properties:\n",
    "        prop = model.properties[prop_name]\n",
    "        print ('\\t',prop_name, ':', prop)\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
