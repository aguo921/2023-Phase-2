{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSA Phase 2 Part 3 - Tune Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect to workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to use Azure ML 1.52.0 to work with MSA-Phase2-Azure\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "# Load the workspace from the saved config file\n",
    "ws = Workspace.from_config()\n",
    "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare a training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder ready.\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "\n",
    "# Create a folder for the training script\n",
    "experiment_folder = 'training-hyperdrive'\n",
    "os.makedirs(experiment_folder, exist_ok=True)\n",
    "\n",
    "shutil.copy('../1. Analysis and Preprocessing/preprocessed_datasets/market_segmentation.csv', os.path.join(experiment_folder, \"market_segmentation.csv\"))\n",
    "\n",
    "print('Folder ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training-hyperdrive/training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $experiment_folder/training.py\n",
    "\n",
    "# Import libraries\n",
    "import argparse, joblib, os\n",
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# Get script arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Set regularization and solver hyperparameters\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--reg_rate', type=float, dest='reg', default=0.01)\n",
    "parser.add_argument('--solver', type=str, dest='solver', default='lbfgs')\n",
    "args = parser.parse_args()\n",
    "reg = args.reg\n",
    "solver = args.solver\n",
    "\n",
    "# Log Hyperparameter values\n",
    "run.log('reg',  np.float(args.reg))\n",
    "run.log('solver', args.solver)\n",
    "\n",
    "# load the market segmentation dataset\n",
    "print(\"Loading Data...\")\n",
    "market_segmentation = pd.read_csv('market_segmentation.csv')\n",
    "\n",
    "# Separate features and labels\n",
    "X, y = market_segmentation.drop(columns=\"Segmentation\"), market_segmentation.Segmentation\n",
    "\n",
    "# Split data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "\n",
    "# Train a logistic regression model\n",
    "print('Training a logistic regression model with regularization rate of', reg)\n",
    "model = LogisticRegression(C=1/reg, solver=solver).fit(X_train, y_train)\n",
    "\n",
    "# Calculate accuracy\n",
    "y_hat = model.predict(X_test)\n",
    "acc = np.average(y_hat == y_test)\n",
    "print('Accuracy:', acc)\n",
    "run.log('Accuracy', np.float(acc))\n",
    "\n",
    "# Calculate AUC\n",
    "label_binarizer = LabelBinarizer().fit(y_train)\n",
    "y_onehot_test = label_binarizer.transform(y_test)\n",
    "y_scores = model.predict_proba(X_test)\n",
    "for class_of_interest in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "    class_id = np.flatnonzero(label_binarizer.classes_ == class_of_interest)[0]\n",
    "    auc = roc_auc_score(y_onehot_test[:,class_id],y_scores[:,class_id])\n",
    "    print(f'AUC {class_of_interest} vs rest: ' + str(auc))\n",
    "    run.log(f'AUC {class_of_interest} vs rest', np.float(auc))\n",
    "\n",
    "# Save the model in the run outputs\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "joblib.dump(value=model, filename='outputs/model.pkl')\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "cluster_name = \"aguo921\"\n",
    "\n",
    "try:\n",
    "    # Check for existing compute target\n",
    "    training_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # If it doesn't already exist, create it\n",
    "    try:\n",
    "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2)\n",
    "        training_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "        training_cluster.wait_for_completion(show_output=True)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run a hyperparameter tuning experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training-hyperdrive/hyperdrive_env.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile $experiment_folder/hyperdrive_env.yml\n",
    "name: batch_environment\n",
    "dependencies:\n",
    "- python=3.6.2\n",
    "- scikit-learn\n",
    "- pandas\n",
    "- numpy\n",
    "- pip\n",
    "- pip:\n",
    "  - azureml-defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'runId': 'HD_2f8eb167-ea06-483f-82cf-4dc30b37c5ca',\n",
       " 'target': 'aguo921',\n",
       " 'status': 'Completed',\n",
       " 'startTimeUtc': '2023-08-01T09:43:30.959306Z',\n",
       " 'endTimeUtc': '2023-08-01T09:48:06.803121Z',\n",
       " 'services': {},\n",
       " 'properties': {'primary_metric_config': '{\"name\":\"Accuracy\",\"goal\":\"maximize\"}',\n",
       "  'resume_from': 'null',\n",
       "  'runTemplate': 'HyperDrive',\n",
       "  'azureml.runsource': 'hyperdrive',\n",
       "  'platform': 'AML',\n",
       "  'ContentSnapshotId': '1f91b411-324e-4cf5-83ca-86bd82e4bf98',\n",
       "  'user_agent': 'python/3.11.3 (Windows-10-10.0.22621-SP0) msrest/0.7.1 Hyperdrive.Service/1.0.0 Hyperdrive.SDK/core.1.52.0',\n",
       "  'space_size': '8',\n",
       "  'score': '0.4968179889690284',\n",
       "  'best_child_run_id': 'HD_2f8eb167-ea06-483f-82cf-4dc30b37c5ca_4',\n",
       "  'best_metric_status': 'Succeeded',\n",
       "  'best_data_container_id': 'dcid.HD_2f8eb167-ea06-483f-82cf-4dc30b37c5ca_4'},\n",
       " 'inputDatasets': [],\n",
       " 'outputDatasets': [],\n",
       " 'runDefinition': {'configuration': None,\n",
       "  'attribution': None,\n",
       "  'telemetryValues': {'amlClientType': 'azureml-sdk-train',\n",
       "   'amlClientModule': '[Scrubbed]',\n",
       "   'amlClientFunction': '[Scrubbed]',\n",
       "   'tenantId': 'd1b36e95-0d50-42e9-958f-b63fa906beaa',\n",
       "   'amlClientRequestId': '17c50aef-e7b9-456d-8710-e55b4a1f5eda',\n",
       "   'amlClientSessionId': 'ac8d61d1-edef-4c40-ae7e-1ee459622192',\n",
       "   'subscriptionId': 'b5ba4903-ea86-4021-ae33-60b2d6e5d120',\n",
       "   'estimator': 'NoneType',\n",
       "   'samplingMethod': 'GRID',\n",
       "   'terminationPolicy': 'Default',\n",
       "   'primaryMetricGoal': 'maximize',\n",
       "   'maxTotalRuns': 12,\n",
       "   'maxConcurrentRuns': 2,\n",
       "   'maxDurationMinutes': 10080,\n",
       "   'vmSize': None},\n",
       "  'snapshotId': '1f91b411-324e-4cf5-83ca-86bd82e4bf98',\n",
       "  'snapshots': [],\n",
       "  'sourceCodeDataReference': None,\n",
       "  'parentRunId': None,\n",
       "  'dataContainerId': None,\n",
       "  'runType': None,\n",
       "  'displayName': None,\n",
       "  'environmentAssetId': None,\n",
       "  'properties': {},\n",
       "  'tags': {},\n",
       "  'aggregatedArtifactPath': None},\n",
       " 'logFiles': {'azureml-logs/hyperdrive.txt': 'https://msaphase2azure3140485650.blob.core.windows.net/azureml/ExperimentRun/dcid.HD_2f8eb167-ea06-483f-82cf-4dc30b37c5ca/azureml-logs/hyperdrive.txt?sv=2019-07-07&sr=b&sig=Q9h2iN7zLRMnqyiTb9neu1GJ93E4%2B560S%2F1%2FJ%2FRtOxY%3D&skoid=03ad897f-d537-417d-a9db-3ae152a154f2&sktid=d1b36e95-0d50-42e9-958f-b63fa906beaa&skt=2023-08-01T08%3A45%3A33Z&ske=2023-08-02T16%3A55%3A33Z&sks=b&skv=2019-07-07&st=2023-08-01T09%3A38%3A12Z&se=2023-08-01T17%3A48%3A12Z&sp=r'},\n",
       " 'submittedBy': 'Angela Guo'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core import Experiment, ScriptRunConfig, Environment\n",
    "from azureml.train.hyperdrive import GridParameterSampling, HyperDriveConfig, PrimaryMetricGoal, choice\n",
    "\n",
    "# Create a Python environment for the experiment\n",
    "hyper_env = Environment.from_conda_specification(\"experiment_env\", experiment_folder + \"/hyperdrive_env.yml\")\n",
    "\n",
    "# Create a script config\n",
    "script_config = ScriptRunConfig(\n",
    "    source_directory=experiment_folder,\n",
    "    script='training.py',\n",
    "    environment=hyper_env,\n",
    "    compute_target = training_cluster\n",
    ")\n",
    "\n",
    "# Sample a range of parameter values\n",
    "params = GridParameterSampling(\n",
    "    {\n",
    "        '--reg': choice(0.3, 0.4, 0.5, 0.6),\n",
    "        '--solver': choice('newton-cg', 'saga')\n",
    "    }\n",
    ")\n",
    "\n",
    "# Configure hyperdrive settings\n",
    "hyperdrive = HyperDriveConfig(\n",
    "    run_config=script_config, \n",
    "    hyperparameter_sampling=params, \n",
    "    policy=None, # No early stopping policy\n",
    "    primary_metric_name='Accuracy', # Find the highest Accuracy metric\n",
    "    primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n",
    "    max_total_runs=12, # Restict the experiment to 16 iterations\n",
    "    max_concurrent_runs=2 # Run up to 2 iterations in parallel\n",
    ") \n",
    "\n",
    "# Run the experiment\n",
    "experiment = Experiment(workspace=ws, name='segmentation-hyperdrive')\n",
    "run = experiment.submit(config=hyperdrive)\n",
    "\n",
    "# Show the status in the notebook as the experiment runs\n",
    "run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Determine the best performing run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'run_id': 'HD_2f8eb167-ea06-483f-82cf-4dc30b37c5ca_5', 'hyperparameters': '{\"--reg\": 0.5, \"--solver\": \"saga\"}', 'best_primary_metric': 0.4968179889690284, 'status': 'Completed'}\n",
      "{'run_id': 'HD_2f8eb167-ea06-483f-82cf-4dc30b37c5ca_4', 'hyperparameters': '{\"--reg\": 0.5, \"--solver\": \"newton-cg\"}', 'best_primary_metric': 0.4968179889690284, 'status': 'Completed'}\n",
      "{'run_id': 'HD_2f8eb167-ea06-483f-82cf-4dc30b37c5ca_7', 'hyperparameters': '{\"--reg\": 0.6, \"--solver\": \"saga\"}', 'best_primary_metric': 0.4963937208315656, 'status': 'Completed'}\n",
      "{'run_id': 'HD_2f8eb167-ea06-483f-82cf-4dc30b37c5ca_6', 'hyperparameters': '{\"--reg\": 0.6, \"--solver\": \"newton-cg\"}', 'best_primary_metric': 0.4963937208315656, 'status': 'Completed'}\n",
      "{'run_id': 'HD_2f8eb167-ea06-483f-82cf-4dc30b37c5ca_2', 'hyperparameters': '{\"--reg\": 0.4, \"--solver\": \"newton-cg\"}', 'best_primary_metric': 0.4959694526941027, 'status': 'Completed'}\n",
      "{'run_id': 'HD_2f8eb167-ea06-483f-82cf-4dc30b37c5ca_3', 'hyperparameters': '{\"--reg\": 0.4, \"--solver\": \"saga\"}', 'best_primary_metric': 0.4959694526941027, 'status': 'Completed'}\n",
      "{'run_id': 'HD_2f8eb167-ea06-483f-82cf-4dc30b37c5ca_1', 'hyperparameters': '{\"--reg\": 0.3, \"--solver\": \"saga\"}', 'best_primary_metric': 0.4955451845566398, 'status': 'Completed'}\n",
      "{'run_id': 'HD_2f8eb167-ea06-483f-82cf-4dc30b37c5ca_0', 'hyperparameters': '{\"--reg\": 0.3, \"--solver\": \"newton-cg\"}', 'best_primary_metric': 0.4955451845566398, 'status': 'Completed'}\n",
      "Best Run Id:  HD_2f8eb167-ea06-483f-82cf-4dc30b37c5ca_4\n",
      " -AUC A vs rest: 0.7173467464996174\n",
      " -AUC B vs rest: 0.6771250706291696\n",
      " -AUC C vs rest: 0.7548471920284576\n",
      " -AUC D vs rest: 0.8663999845277084\n",
      " -Accuracy: 0.4968179889690284\n",
      " -Arguments: ['--reg', '0.5', '--solver', 'newton-cg']\n"
     ]
    }
   ],
   "source": [
    "# Print all child runs, sorted by the primary metric\n",
    "for child_run in run.get_children_sorted_by_primary_metric():\n",
    "    print(child_run)\n",
    "\n",
    "# Get the best run, and its metrics and arguments\n",
    "best_run = run.get_best_run_by_primary_metric()\n",
    "best_run_metrics = best_run.get_metrics()\n",
    "script_arguments = best_run.get_details() ['runDefinition']['arguments']\n",
    "print('Best Run Id: ', best_run.id)\n",
    "print(' -AUC A vs rest:', best_run_metrics['AUC A vs rest'])\n",
    "print(' -AUC B vs rest:', best_run_metrics['AUC B vs rest'])\n",
    "print(' -AUC C vs rest:', best_run_metrics['AUC C vs rest'])\n",
    "print(' -AUC D vs rest:', best_run_metrics['AUC D vs rest'])\n",
    "print(' -Accuracy:', best_run_metrics['Accuracy'])\n",
    "print(' -Arguments:',script_arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segmentation-model version: 5\n",
      "\t Training context : Hyperdrive\n",
      "\t AUC A vs rest : 0.7173467464996174\n",
      "\t AUC B vs rest : 0.6771250706291696\n",
      "\t AUC C vs rest : 0.7548471920284576\n",
      "\t AUC D vs rest : 0.8663999845277084\n",
      "\t Accuracy : 0.4968179889690284\n",
      "\n",
      "\n",
      "segmentation-model version: 4\n",
      "\t Training context : Hyperdrive\n",
      "\t AUC A vs rest : 0.7173467464996174\n",
      "\t AUC B vs rest : 0.6771250706291696\n",
      "\t AUC C vs rest : 0.7548471920284576\n",
      "\t AUC D vs rest : 0.8663999845277084\n",
      "\t Accuracy : 0.4968179889690284\n",
      "\n",
      "\n",
      "segmentation-model version: 3\n",
      "\t Training context : Hyperdrive\n",
      "\t AUC A vs rest : 0.7111122006745038\n",
      "\t AUC B vs rest : 0.6643216130215407\n",
      "\t AUC C vs rest : 0.7541089423407565\n",
      "\t AUC D vs rest : 0.8669461195860793\n",
      "\t Accuracy : 0.49087823504454814\n",
      "\n",
      "\n",
      "segmentation-logistic-regression version: 2\n",
      "\n",
      "\n",
      "segmentation-model version: 2\n",
      "\t Training context : Script\n",
      "\t AUC A vs rest : 0.7113390862280469\n",
      "\t AUC B vs rest : 0.6641015473983684\n",
      "\t AUC C vs rest : 0.754124484439445\n",
      "\t AUC D vs rest : 0.8668678371578138\n",
      "\t Accuracy : 0.4900296987696224\n",
      "\n",
      "\n",
      "segmentation-model version: 1\n",
      "\t Training context : Script\n",
      "\t AUC A vs rest : 0.7113390862280469\n",
      "\t AUC B vs rest : 0.6641015473983684\n",
      "\t AUC C vs rest : 0.754124484439445\n",
      "\t AUC D vs rest : 0.8668678371578138\n",
      "\t Accuracy : 0.4900296987696224\n",
      "\n",
      "\n",
      "segmentation-logistic-regression version: 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Model\n",
    "\n",
    "# Register model\n",
    "best_run.register_model(\n",
    "    model_path='outputs/model.pkl', model_name='segmentation-model',\n",
    "    tags={'Training context':'Hyperdrive'},\n",
    "    properties={\n",
    "        'AUC A vs rest': best_run_metrics['AUC A vs rest'],\n",
    "        'AUC B vs rest': best_run_metrics['AUC B vs rest'],\n",
    "        'AUC C vs rest': best_run_metrics['AUC C vs rest'],\n",
    "        'AUC D vs rest': best_run_metrics['AUC D vs rest'],\n",
    "        'Accuracy': best_run_metrics['Accuracy']\n",
    "    }\n",
    ")\n",
    "\n",
    "# List registered models\n",
    "for model in Model.list(ws):\n",
    "    print(model.name, 'version:', model.version)\n",
    "    for tag_name in model.tags:\n",
    "        tag = model.tags[tag_name]\n",
    "        print ('\\t',tag_name, ':', tag)\n",
    "    for prop_name in model.properties:\n",
    "        prop = model.properties[prop_name]\n",
    "        print ('\\t',prop_name, ':', prop)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Screenshots of final run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are screenshots of the Azure interface from my final hyperparameter tuning run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/accuracy-vs-time.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the best child over time as we sample different sets of hyperparameters improves over time as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/parallel-coordinates.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a parallel coordinates chart showing the effect of the regularisation rate and solver on the accuracy of the model. We find that the newton-cg and saga solver give the same accuracy and that a regularisation rate of 0.5 gives the best accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/best-model-metrics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best hyperparameter tuned model has an accuracy of almost 50%, which means it correctly segments almost half of the customers. It has the highest AUC score when classifying customers in group D and the lowest AUC score when classifying customers in group B."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
