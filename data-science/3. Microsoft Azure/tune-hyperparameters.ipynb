{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect to workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "# Load the workspace from the saved config file\n",
    "ws = Workspace.from_config()\n",
    "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare a training script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperdrive Training Folder Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "\n",
    "# Create a folder for the training script\n",
    "experiment_folder = 'training-hyperdrive'\n",
    "os.makedirs(experiment_folder, exist_ok=True)\n",
    "\n",
    "shutil.copy(\n",
    "    '../1. Analysis and Preprocessing/preprocessed_datasets/market_segmentation.csv',\n",
    "    os.path.join(experiment_folder, \"market_segmentation.csv\")\n",
    ")\n",
    "shutil.copy(\n",
    "    '../1. Analysis and Preprocessing/preprocessed_datasets/market_segmentation_interaction.csv',\n",
    "    os.path.join(experiment_folder, \"market_segmentation_interaction.csv\")\n",
    ")\n",
    "\n",
    "print('Folder ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $experiment_folder/lg-training.py\n",
    "\n",
    "# Import libraries\n",
    "import argparse, joblib, os\n",
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# Get script arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Set regularization and solver hyperparameters\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--reg_rate', type=float, dest='reg', default=0.01)\n",
    "parser.add_argument('--solver', type=str, dest='solver', default='lbfgs')\n",
    "args = parser.parse_args()\n",
    "reg = args.reg\n",
    "solver = args.solver\n",
    "\n",
    "# Log Hyperparameter values\n",
    "run.log('reg',  np.float(args.reg))\n",
    "run.log('solver', args.solver)\n",
    "\n",
    "# load the market segmentation dataset\n",
    "print(\"Loading Data...\")\n",
    "market_segmentation = pd.read_csv('market_segmentation_interaction.csv')\n",
    "\n",
    "# Separate features and labels\n",
    "X, y = market_segmentation.drop(columns=\"Segmentation\"), market_segmentation.Segmentation\n",
    "\n",
    "# Split data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "\n",
    "# Train a logistic regression model\n",
    "print('Training a logistic regression model with regularization rate of', reg)\n",
    "model = LogisticRegression(C=1/reg, solver=solver).fit(X_train, y_train)\n",
    "\n",
    "# Calculate accuracy\n",
    "y_hat = model.predict(X_test)\n",
    "acc = np.average(y_hat == y_test)\n",
    "print('Accuracy:', acc)\n",
    "run.log('Accuracy', np.float(acc))\n",
    "\n",
    "# Calculate AUC\n",
    "label_binarizer = LabelBinarizer().fit(y_train)\n",
    "y_onehot_test = label_binarizer.transform(y_test)\n",
    "y_scores = model.predict_proba(X_test)\n",
    "for class_of_interest in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "    class_id = np.flatnonzero(label_binarizer.classes_ == class_of_interest)[0]\n",
    "    auc = roc_auc_score(y_onehot_test[:,class_id],y_scores[:,class_id])\n",
    "    print(f'AUC {class_of_interest} vs rest: ' + str(auc))\n",
    "    run.log(f'AUC {class_of_interest} vs rest', np.float(auc))\n",
    "\n",
    "# Save the model in the run outputs\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "joblib.dump(value=model, filename='outputs/lg-model.pkl')\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $experiment_folder/rf-training.py\n",
    "\n",
    "# Import libraries\n",
    "import argparse, joblib, os\n",
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# Get script arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Set hyperparameters\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--max_features', type=int, dest='max_features', default=4)\n",
    "parser.add_argument('--max_depth', type=int, dest='max_depth', default=None)\n",
    "parser.add_argument('--min_samples_split', type=int, dest='min_samples_split', default=2)\n",
    "parser.add_argument('--min_samples_leaf', type=int, dest='min_samples_leaf', default=1)\n",
    "args = parser.parse_args()\n",
    "max_features = args.max_features\n",
    "max_depth = args.max_depth\n",
    "min_samples_split = args.min_samples_split\n",
    "min_samples_leaf = args.min_samples_leaf\n",
    "\n",
    "# Log Hyperparameter values\n",
    "run.log('Maximum depth of tree', max_depth)\n",
    "run.log('Maximum features per split', max_features)\n",
    "run.log('Minimum samples requierd for split', min_samples_split)\n",
    "run.log('Minimum samples per leaf', min_samples_leaf)\n",
    "\n",
    "# load the market segmentation dataset\n",
    "print(\"Loading Data...\")\n",
    "market_segmentation = pd.read_csv('market_segmentation.csv')\n",
    "\n",
    "# Separate features and labels\n",
    "X, y = market_segmentation.drop(columns=\"Segmentation\"), market_segmentation.Segmentation\n",
    "\n",
    "# Split data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "\n",
    "# Train a logistic regression model\n",
    "print(\n",
    "    'Training a random forest model with max features of', max_features,\n",
    "    'max depth of', max_depth,\n",
    "    'min samples split of', min_samples_split,\n",
    "    'and min_samples_leaf of', min_samples_leaf\n",
    ")\n",
    "model = RandomForestClassifier(\n",
    "    max_features=max_features,\n",
    "    max_depth=max_depth,\n",
    "    min_samples_split=min_samples_split,\n",
    "    min_samples_leaf=min_samples_leaf\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "# Calculate accuracy\n",
    "y_hat = model.predict(X_test)\n",
    "acc = np.average(y_hat == y_test)\n",
    "print('Accuracy:', acc)\n",
    "run.log('Accuracy', np.float(acc))\n",
    "\n",
    "# Calculate AUC\n",
    "label_binarizer = LabelBinarizer().fit(y_train)\n",
    "y_onehot_test = label_binarizer.transform(y_test)\n",
    "y_scores = model.predict_proba(X_test)\n",
    "for class_of_interest in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "    class_id = np.flatnonzero(label_binarizer.classes_ == class_of_interest)[0]\n",
    "    auc = roc_auc_score(y_onehot_test[:,class_id],y_scores[:,class_id])\n",
    "    print(f'AUC {class_of_interest} vs rest: ' + str(auc))\n",
    "    run.log(f'AUC {class_of_interest} vs rest', np.float(auc))\n",
    "\n",
    "# Save the model in the run outputs\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "joblib.dump(value=model, filename='outputs/rf-model.pkl')\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "cluster_name = \"aguo921\"\n",
    "\n",
    "try:\n",
    "    # Check for existing compute target\n",
    "    training_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # If it doesn't already exist, create it\n",
    "    try:\n",
    "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2)\n",
    "        training_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "        training_cluster.wait_for_completion(show_output=True)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $experiment_folder/hyperdrive_env.yml\n",
    "name: batch_environment\n",
    "dependencies:\n",
    "- python=3.6.2\n",
    "- scikit-learn\n",
    "- pandas\n",
    "- numpy\n",
    "- pip\n",
    "- pip:\n",
    "  - azureml-defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment, ScriptRunConfig, Environment\n",
    "from azureml.train.hyperdrive import GridParameterSampling, HyperDriveConfig, PrimaryMetricGoal, choice\n",
    "\n",
    "# Create a Python environment for the experiment\n",
    "hyper_env = Environment.from_conda_specification(\"experiment_env\", experiment_folder + \"/hyperdrive_env.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run a hyperparameter tuning experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a script config\n",
    "script_config = ScriptRunConfig(\n",
    "    source_directory=experiment_folder,\n",
    "    script='lg-training.py',\n",
    "    environment=hyper_env,\n",
    "    compute_target = training_cluster\n",
    ")\n",
    "\n",
    "# Sample a range of parameter values\n",
    "params = GridParameterSampling(\n",
    "    {\n",
    "        '--reg': choice(0.3, 0.4, 0.5, 0.6),\n",
    "        '--solver': choice('newton-cg', 'lbfgs')\n",
    "    }\n",
    ")\n",
    "\n",
    "# Configure hyperdrive settings\n",
    "hyperdrive = HyperDriveConfig(\n",
    "    run_config=script_config, \n",
    "    hyperparameter_sampling=params, \n",
    "    policy=None, # No early stopping policy\n",
    "    primary_metric_name='Accuracy', # Find the highest Accuracy metric\n",
    "    primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n",
    "    max_total_runs=12, # Restict the experiment to 16 iterations\n",
    "    max_concurrent_runs=2 # Run up to 2 iterations in parallel\n",
    ") \n",
    "\n",
    "# Run the experiment\n",
    "experiment = Experiment(workspace=ws, name='segmentation-hyperdrive-lg')\n",
    "run_lg = experiment.submit(config=hyperdrive)\n",
    "\n",
    "# Show the status in the notebook as the experiment runs\n",
    "run_lg.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a script config\n",
    "script_config = ScriptRunConfig(\n",
    "    source_directory=experiment_folder,\n",
    "    script='rf-training.py',\n",
    "    environment=hyper_env,\n",
    "    compute_target = training_cluster\n",
    ")\n",
    "\n",
    "# Sample a range of parameter values\n",
    "params = GridParameterSampling(\n",
    "    {\n",
    "        '--max_features': choice(3),\n",
    "        '--max_depth': choice(12, 15),\n",
    "        '--min_samples_split': choice(10, 11),\n",
    "        '--min_samples_leaf': choice(11, 12)\n",
    "    }\n",
    ")\n",
    "\n",
    "# Configure hyperdrive settings\n",
    "hyperdrive = HyperDriveConfig(\n",
    "    run_config=script_config, \n",
    "    hyperparameter_sampling=params, \n",
    "    policy=None, # No early stopping policy\n",
    "    primary_metric_name='Accuracy', # Find the highest Accuracy metric\n",
    "    primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n",
    "    max_total_runs=16, # Restict the experiment to 10 iterations\n",
    "    max_concurrent_runs=2 # Run up to 2 iterations in parallel\n",
    ") \n",
    "\n",
    "# Run the experiment\n",
    "experiment = Experiment(workspace=ws, name='segmentation-hyperdrive-rf')\n",
    "run_rf = experiment.submit(config=hyperdrive)\n",
    "\n",
    "# Show the status in the notebook as the experiment runs\n",
    "run_rf.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Register the best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all child runs, sorted by the primary metric\n",
    "for child_run in run_lg.get_children_sorted_by_primary_metric():\n",
    "    print(child_run)\n",
    "\n",
    "# Get the best run, and its metrics and arguments\n",
    "best_run = run_lg.get_best_run_by_primary_metric()\n",
    "best_run_metrics = best_run.get_metrics()\n",
    "script_arguments = best_run.get_details() ['runDefinition']['arguments']\n",
    "print('Best Run Id: ', best_run.id)\n",
    "print(' -AUC A vs rest:', best_run_metrics['AUC A vs rest'])\n",
    "print(' -AUC B vs rest:', best_run_metrics['AUC B vs rest'])\n",
    "print(' -AUC C vs rest:', best_run_metrics['AUC C vs rest'])\n",
    "print(' -AUC D vs rest:', best_run_metrics['AUC D vs rest'])\n",
    "print(' -Accuracy:', best_run_metrics['Accuracy'])\n",
    "print(' -Arguments:',script_arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register model\n",
    "best_run.register_model(\n",
    "    model_path='outputs/lg-model.pkl', model_name='segmentation-lg',\n",
    "    tags={'Training context':'Hyperdrive'},\n",
    "    properties={\n",
    "        'AUC A vs rest': best_run_metrics['AUC A vs rest'],\n",
    "        'AUC B vs rest': best_run_metrics['AUC B vs rest'],\n",
    "        'AUC C vs rest': best_run_metrics['AUC C vs rest'],\n",
    "        'AUC D vs rest': best_run_metrics['AUC D vs rest'],\n",
    "        'Accuracy': best_run_metrics['Accuracy']\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all child runs, sorted by the primary metric\n",
    "for child_run in run_rf.get_children_sorted_by_primary_metric():\n",
    "    print(child_run)\n",
    "\n",
    "# Get the best run, and its metrics and arguments\n",
    "best_run = run_rf.get_best_run_by_primary_metric()\n",
    "best_run_metrics = best_run.get_metrics()\n",
    "script_arguments = best_run.get_details() ['runDefinition']['arguments']\n",
    "print('Best Run Id: ', best_run.id)\n",
    "print(' -AUC A vs rest:', best_run_metrics['AUC A vs rest'])\n",
    "print(' -AUC B vs rest:', best_run_metrics['AUC B vs rest'])\n",
    "print(' -AUC C vs rest:', best_run_metrics['AUC C vs rest'])\n",
    "print(' -AUC D vs rest:', best_run_metrics['AUC D vs rest'])\n",
    "print(' -Accuracy:', best_run_metrics['Accuracy'])\n",
    "print(' -Arguments:',script_arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register model\n",
    "best_run.register_model(\n",
    "    model_path='outputs/rf-model.pkl', model_name='segmentation-rf',\n",
    "    tags={'Training context':'Hyperdrive'},\n",
    "    properties={\n",
    "        'AUC A vs rest': best_run_metrics['AUC A vs rest'],\n",
    "        'AUC B vs rest': best_run_metrics['AUC B vs rest'],\n",
    "        'AUC C vs rest': best_run_metrics['AUC C vs rest'],\n",
    "        'AUC D vs rest': best_run_metrics['AUC D vs rest'],\n",
    "        'Accuracy': best_run_metrics['Accuracy']\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List registered models\n",
    "for model in Model.list(ws):\n",
    "    print(model.name, 'version:', model.version)\n",
    "    for tag_name in model.tags:\n",
    "        tag = model.tags[tag_name]\n",
    "        print ('\\t',tag_name, ':', tag)\n",
    "    for prop_name in model.properties:\n",
    "        prop = model.properties[prop_name]\n",
    "        print ('\\t',prop_name, ':', prop)\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
