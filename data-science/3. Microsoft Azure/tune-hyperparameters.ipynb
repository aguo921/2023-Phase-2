{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect to workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: Azure ML was unable to load cloud metadata from the url specified by https://management.azure.com/metadata/endpoints?api-version=2019-05-01. HTTPSConnectionPool(host='management.azure.com', port=443): Max retries exceeded with url: /metadata/endpoints?api-version=2019-05-01 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001842A0AA710>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')). This may be due to a misconfiguration of networking controls. Azure Machine Learning Python SDK requires outbound access to Azure Resource Manager. Please contact your networking team to configure outbound access to Azure Resource Manager on both Network Security Group and Firewall. For more details on required configurations, see https://docs.microsoft.com/azure/machine-learning/how-to-access-azureml-behind-firewall.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to use Azure ML 1.52.0 to work with MSA-Phase2-Azure\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "# Load the workspace from the saved config file\n",
    "ws = Workspace.from_config()\n",
    "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare a training script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperdrive Training Folder Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder ready.\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "\n",
    "# Create a folder for the training script\n",
    "experiment_folder = 'training-hyperdrive'\n",
    "os.makedirs(experiment_folder, exist_ok=True)\n",
    "\n",
    "shutil.copy(\n",
    "    '../1. Analysis and Preprocessing/preprocessed_datasets/market_segmentation.csv',\n",
    "    os.path.join(experiment_folder, \"market_segmentation.csv\")\n",
    ")\n",
    "shutil.copy(\n",
    "    '../1. Analysis and Preprocessing/preprocessed_datasets/market_segmentation_interaction.csv',\n",
    "    os.path.join(experiment_folder, \"market_segmentation_interaction.csv\")\n",
    ")\n",
    "\n",
    "print('Folder ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $experiment_folder/lg-training.py\n",
    "\n",
    "# Import libraries\n",
    "import argparse, joblib, os\n",
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# Get script arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Set regularization and solver hyperparameters\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--reg_rate', type=float, dest='reg', default=0.01)\n",
    "parser.add_argument('--solver', type=str, dest='solver', default='lbfgs')\n",
    "args = parser.parse_args()\n",
    "reg = args.reg\n",
    "solver = args.solver\n",
    "\n",
    "# Log Hyperparameter values\n",
    "run.log('reg',  np.float(args.reg))\n",
    "run.log('solver', args.solver)\n",
    "\n",
    "# load the market segmentation dataset\n",
    "print(\"Loading Data...\")\n",
    "market_segmentation = pd.read_csv('market_segmentation_interaction.csv')\n",
    "\n",
    "# Separate features and labels\n",
    "X, y = market_segmentation.drop(columns=\"Segmentation\"), market_segmentation.Segmentation\n",
    "\n",
    "# Split data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "\n",
    "# Train a logistic regression model\n",
    "print('Training a logistic regression model with regularization rate of', reg)\n",
    "model = LogisticRegression(C=1/reg, solver=solver).fit(X_train, y_train)\n",
    "\n",
    "# Calculate accuracy\n",
    "y_hat = model.predict(X_test)\n",
    "acc = np.average(y_hat == y_test)\n",
    "print('Accuracy:', acc)\n",
    "run.log('Accuracy', np.float(acc))\n",
    "\n",
    "# Calculate AUC\n",
    "label_binarizer = LabelBinarizer().fit(y_train)\n",
    "y_onehot_test = label_binarizer.transform(y_test)\n",
    "y_scores = model.predict_proba(X_test)\n",
    "for class_of_interest in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "    class_id = np.flatnonzero(label_binarizer.classes_ == class_of_interest)[0]\n",
    "    auc = roc_auc_score(y_onehot_test[:,class_id],y_scores[:,class_id])\n",
    "    print(f'AUC {class_of_interest} vs rest: ' + str(auc))\n",
    "    run.log(f'AUC {class_of_interest} vs rest', np.float(auc))\n",
    "\n",
    "# Save the model in the run outputs\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "joblib.dump(value=model, filename='outputs/lg-model.pkl')\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $experiment_folder/rf-training.py\n",
    "\n",
    "# Import libraries\n",
    "import argparse, joblib, os\n",
    "from azureml.core import Run\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# Get script arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Set hyperparameters\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--max_features', type=int, dest='max_features', default=4)\n",
    "parser.add_argument('--max_depth', type=int, dest='max_depth', default=None)\n",
    "parser.add_argument('--min_samples_split', type=int, dest='min_samples_split', default=2)\n",
    "parser.add_argument('--min_samples_leaf', type=int, dest='min_samples_leaf', default=1)\n",
    "args = parser.parse_args()\n",
    "max_features = args.max_features\n",
    "max_depth = args.max_depth\n",
    "min_samples_split = args.min_samples_split\n",
    "min_samples_leaf = args.min_samples_leaf\n",
    "\n",
    "# Log Hyperparameter values\n",
    "run.log('Maximum depth of tree', max_depth)\n",
    "run.log('Maximum features per split', max_features)\n",
    "run.log('Minimum samples requierd for split', min_samples_split)\n",
    "run.log('Minimum samples per leaf', min_samples_leaf)\n",
    "\n",
    "# load the market segmentation dataset\n",
    "print(\"Loading Data...\")\n",
    "market_segmentation = pd.read_csv('market_segmentation.csv')\n",
    "\n",
    "# Separate features and labels\n",
    "X, y = market_segmentation.drop(columns=\"Segmentation\"), market_segmentation.Segmentation\n",
    "\n",
    "# Split data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "\n",
    "# Train a logistic regression model\n",
    "print(\n",
    "    'Training a random forest model with max features of', max_features,\n",
    "    'max depth of', max_depth,\n",
    "    'min samples split of', min_samples_split,\n",
    "    'and min_samples_leaf of', min_samples_leaf\n",
    ")\n",
    "model = RandomForestClassifier(\n",
    "    max_features=max_features,\n",
    "    max_depth=max_depth,\n",
    "    min_samples_split=min_samples_split,\n",
    "    min_samples_leaf=min_samples_leaf\n",
    ").fit(X_train, y_train)\n",
    "\n",
    "# Calculate accuracy\n",
    "y_hat = model.predict(X_test)\n",
    "acc = np.average(y_hat == y_test)\n",
    "print('Accuracy:', acc)\n",
    "run.log('Accuracy', np.float(acc))\n",
    "\n",
    "# Calculate AUC\n",
    "label_binarizer = LabelBinarizer().fit(y_train)\n",
    "y_onehot_test = label_binarizer.transform(y_test)\n",
    "y_scores = model.predict_proba(X_test)\n",
    "for class_of_interest in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "    class_id = np.flatnonzero(label_binarizer.classes_ == class_of_interest)[0]\n",
    "    auc = roc_auc_score(y_onehot_test[:,class_id],y_scores[:,class_id])\n",
    "    print(f'AUC {class_of_interest} vs rest: ' + str(auc))\n",
    "    run.log(f'AUC {class_of_interest} vs rest', np.float(auc))\n",
    "\n",
    "# Save the model in the run outputs\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "joblib.dump(value=model, filename='outputs/rf-model.pkl')\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "cluster_name = \"aguo921\"\n",
    "\n",
    "try:\n",
    "    # Check for existing compute target\n",
    "    training_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # If it doesn't already exist, create it\n",
    "    try:\n",
    "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2)\n",
    "        training_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "        training_cluster.wait_for_completion(show_output=True)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $experiment_folder/hyperdrive_env.yml\n",
    "name: batch_environment\n",
    "dependencies:\n",
    "- python=3.6.2\n",
    "- scikit-learn\n",
    "- pandas\n",
    "- numpy\n",
    "- pip\n",
    "- pip:\n",
    "  - azureml-defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment, ScriptRunConfig, Environment\n",
    "from azureml.train.hyperdrive import GridParameterSampling, HyperDriveConfig, PrimaryMetricGoal, choice\n",
    "\n",
    "# Create a Python environment for the experiment\n",
    "hyper_env = Environment.from_conda_specification(\"experiment_env\", experiment_folder + \"/hyperdrive_env.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run a hyperparameter tuning experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'runId': 'HD_6047097b-1b93-4420-a7f2-4280aed9df60',\n",
       " 'target': 'aguo921',\n",
       " 'status': 'Completed',\n",
       " 'startTimeUtc': '2023-08-02T20:50:10.396357Z',\n",
       " 'endTimeUtc': '2023-08-02T20:57:14.6008Z',\n",
       " 'services': {},\n",
       " 'properties': {'primary_metric_config': '{\"name\":\"Accuracy\",\"goal\":\"maximize\"}',\n",
       "  'resume_from': 'null',\n",
       "  'runTemplate': 'HyperDrive',\n",
       "  'azureml.runsource': 'hyperdrive',\n",
       "  'platform': 'AML',\n",
       "  'ContentSnapshotId': '06af74e7-b762-4097-9125-1f94eb14eeda',\n",
       "  'user_agent': 'python/3.11.3 (Windows-10-10.0.22621-SP0) msrest/0.7.1 Hyperdrive.Service/1.0.0 Hyperdrive.SDK/core.1.52.0',\n",
       "  'space_size': '9',\n",
       "  'score': '0.4997696913864578',\n",
       "  'best_child_run_id': 'HD_6047097b-1b93-4420-a7f2-4280aed9df60_3',\n",
       "  'best_metric_status': 'Succeeded',\n",
       "  'best_data_container_id': 'dcid.HD_6047097b-1b93-4420-a7f2-4280aed9df60_3'},\n",
       " 'inputDatasets': [],\n",
       " 'outputDatasets': [],\n",
       " 'runDefinition': {'configuration': None,\n",
       "  'attribution': None,\n",
       "  'telemetryValues': {'amlClientType': 'azureml-sdk-train',\n",
       "   'amlClientModule': '[Scrubbed]',\n",
       "   'amlClientFunction': '[Scrubbed]',\n",
       "   'tenantId': 'd1b36e95-0d50-42e9-958f-b63fa906beaa',\n",
       "   'amlClientRequestId': '2a3a3003-d442-49aa-91ac-a228ecfe0cf9',\n",
       "   'amlClientSessionId': 'efbd6a42-0474-41b9-9140-b1f3625b6e38',\n",
       "   'subscriptionId': 'b5ba4903-ea86-4021-ae33-60b2d6e5d120',\n",
       "   'estimator': 'NoneType',\n",
       "   'samplingMethod': 'GRID',\n",
       "   'terminationPolicy': 'Default',\n",
       "   'primaryMetricGoal': 'maximize',\n",
       "   'maxTotalRuns': 16,\n",
       "   'maxConcurrentRuns': 2,\n",
       "   'maxDurationMinutes': 10080,\n",
       "   'vmSize': None},\n",
       "  'snapshotId': '06af74e7-b762-4097-9125-1f94eb14eeda',\n",
       "  'snapshots': [],\n",
       "  'sourceCodeDataReference': None,\n",
       "  'parentRunId': None,\n",
       "  'dataContainerId': None,\n",
       "  'runType': None,\n",
       "  'displayName': None,\n",
       "  'environmentAssetId': None,\n",
       "  'properties': {},\n",
       "  'tags': {},\n",
       "  'aggregatedArtifactPath': None},\n",
       " 'logFiles': {'azureml-logs/hyperdrive.txt': 'https://msaphase2azure3140485650.blob.core.windows.net/azureml/ExperimentRun/dcid.HD_6047097b-1b93-4420-a7f2-4280aed9df60/azureml-logs/hyperdrive.txt?sv=2019-07-07&sr=b&sig=YlnRbUL%2B5I9BeeHvbF1twEvPGAVqUv4omgW1R6tbXew%3D&skoid=03ad897f-d537-417d-a9db-3ae152a154f2&sktid=d1b36e95-0d50-42e9-958f-b63fa906beaa&skt=2023-08-02T20%3A15%3A37Z&ske=2023-08-04T04%3A25%3A37Z&sks=b&skv=2019-07-07&st=2023-08-02T20%3A47%3A59Z&se=2023-08-03T04%3A57%3A59Z&sp=r'},\n",
       " 'submittedBy': 'Angela Guo'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a script config\n",
    "script_config = ScriptRunConfig(\n",
    "    source_directory=experiment_folder,\n",
    "    script='lg-training.py',\n",
    "    environment=hyper_env,\n",
    "    compute_target = training_cluster\n",
    ")\n",
    "\n",
    "# Sample a range of parameter values\n",
    "params = GridParameterSampling(\n",
    "    {\n",
    "        '--reg': choice(0.5, 1, 2),\n",
    "        '--solver': choice('newton-cg', 'lbfgs', 'saga')\n",
    "    }\n",
    ")\n",
    "\n",
    "# Configure hyperdrive settings\n",
    "hyperdrive = HyperDriveConfig(\n",
    "    run_config=script_config, \n",
    "    hyperparameter_sampling=params, \n",
    "    policy=None, # No early stopping policy\n",
    "    primary_metric_name='Accuracy', # Find the highest Accuracy metric\n",
    "    primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n",
    "    max_total_runs=16, # Restict the experiment to 16 iterations\n",
    "    max_concurrent_runs=2 # Run up to 2 iterations in parallel\n",
    ") \n",
    "\n",
    "# Run the experiment\n",
    "experiment = Experiment(workspace=ws, name='segmentation-hyperdrive-lg')\n",
    "run_lg = experiment.submit(config=hyperdrive)\n",
    "\n",
    "# Show the status in the notebook as the experiment runs\n",
    "run_lg.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a script config\n",
    "script_config = ScriptRunConfig(\n",
    "    source_directory=experiment_folder,\n",
    "    script='rf-training.py',\n",
    "    environment=hyper_env,\n",
    "    compute_target = training_cluster\n",
    ")\n",
    "\n",
    "# Sample a range of parameter values\n",
    "params = GridParameterSampling(\n",
    "    {\n",
    "        '--max_features': choice(3),\n",
    "        '--max_depth': choice(12, 15),\n",
    "        '--min_samples_split': choice(10, 11),\n",
    "        '--min_samples_leaf': choice(11, 12)\n",
    "    }\n",
    ")\n",
    "\n",
    "# Configure hyperdrive settings\n",
    "hyperdrive = HyperDriveConfig(\n",
    "    run_config=script_config, \n",
    "    hyperparameter_sampling=params, \n",
    "    policy=None, # No early stopping policy\n",
    "    primary_metric_name='Accuracy', # Find the highest Accuracy metric\n",
    "    primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n",
    "    max_total_runs=16, # Restict the experiment to 10 iterations\n",
    "    max_concurrent_runs=2 # Run up to 2 iterations in parallel\n",
    ") \n",
    "\n",
    "# Run the experiment\n",
    "experiment = Experiment(workspace=ws, name='segmentation-hyperdrive-rf')\n",
    "run_rf = experiment.submit(config=hyperdrive)\n",
    "\n",
    "# Show the status in the notebook as the experiment runs\n",
    "run_rf.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Register the best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'run_id': 'HD_6047097b-1b93-4420-a7f2-4280aed9df60_3', 'hyperparameters': '{\"--reg\": 1, \"--solver\": \"lbfgs\"}', 'best_primary_metric': 0.4997696913864578, 'status': 'Completed'}\n",
      "{'run_id': 'HD_6047097b-1b93-4420-a7f2-4280aed9df60_4', 'hyperparameters': '{\"--reg\": 1, \"--solver\": \"newton-cg\"}', 'best_primary_metric': 0.49884845693228924, 'status': 'Completed'}\n",
      "{'run_id': 'HD_6047097b-1b93-4420-a7f2-4280aed9df60_5', 'hyperparameters': '{\"--reg\": 1, \"--solver\": \"saga\"}', 'best_primary_metric': 0.49884845693228924, 'status': 'Completed'}\n",
      "{'run_id': 'HD_6047097b-1b93-4420-a7f2-4280aed9df60_6', 'hyperparameters': '{\"--reg\": 2, \"--solver\": \"lbfgs\"}', 'best_primary_metric': 0.49792722247812066, 'status': 'Completed'}\n",
      "{'run_id': 'HD_6047097b-1b93-4420-a7f2-4280aed9df60_8', 'hyperparameters': '{\"--reg\": 2, \"--solver\": \"saga\"}', 'best_primary_metric': 0.4965453707968678, 'status': 'Completed'}\n",
      "{'run_id': 'HD_6047097b-1b93-4420-a7f2-4280aed9df60_7', 'hyperparameters': '{\"--reg\": 2, \"--solver\": \"newton-cg\"}', 'best_primary_metric': 0.4965453707968678, 'status': 'Completed'}\n",
      "{'run_id': 'HD_6047097b-1b93-4420-a7f2-4280aed9df60_2', 'hyperparameters': '{\"--reg\": 0.5, \"--solver\": \"saga\"}', 'best_primary_metric': 0.4960847535697835, 'status': 'Completed'}\n",
      "{'run_id': 'HD_6047097b-1b93-4420-a7f2-4280aed9df60_1', 'hyperparameters': '{\"--reg\": 0.5, \"--solver\": \"newton-cg\"}', 'best_primary_metric': 0.4960847535697835, 'status': 'Completed'}\n",
      "{'run_id': 'HD_6047097b-1b93-4420-a7f2-4280aed9df60_0', 'hyperparameters': '{\"--reg\": 0.5, \"--solver\": \"lbfgs\"}', 'best_primary_metric': 0.4960847535697835, 'status': 'Completed'}\n",
      "Best Run Id:  HD_6047097b-1b93-4420-a7f2-4280aed9df60_3\n",
      " -AUC A vs rest: 0.6939644824258878\n",
      " -AUC B vs rest: 0.6825979464106733\n",
      " -AUC C vs rest: 0.758906343300721\n",
      " -AUC D vs rest: 0.876593162609928\n",
      " -Accuracy: 0.4997696913864578\n",
      " -Arguments: ['--reg', '1', '--solver', 'lbfgs']\n"
     ]
    }
   ],
   "source": [
    "# Print all child runs, sorted by the primary metric\n",
    "for child_run in run_lg.get_children_sorted_by_primary_metric():\n",
    "    print(child_run)\n",
    "\n",
    "# Get the best run, and its metrics and arguments\n",
    "best_run = run_lg.get_best_run_by_primary_metric()\n",
    "best_run_metrics = best_run.get_metrics()\n",
    "script_arguments = best_run.get_details() ['runDefinition']['arguments']\n",
    "print('Best Run Id: ', best_run.id)\n",
    "print(' -AUC A vs rest:', best_run_metrics['AUC A vs rest'])\n",
    "print(' -AUC B vs rest:', best_run_metrics['AUC B vs rest'])\n",
    "print(' -AUC C vs rest:', best_run_metrics['AUC C vs rest'])\n",
    "print(' -AUC D vs rest:', best_run_metrics['AUC D vs rest'])\n",
    "print(' -Accuracy:', best_run_metrics['Accuracy'])\n",
    "print(' -Arguments:',script_arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(workspace=Workspace.create(name='MSA-Phase2-Azure', subscription_id='b5ba4903-ea86-4021-ae33-60b2d6e5d120', resource_group='MSA-Phase2-Azure'), name=segmentation-lg, id=segmentation-lg:4, version=4, tags={'Training context': 'Hyperdrive'}, properties={'AUC A vs rest': '0.6939644824258878', 'AUC B vs rest': '0.6825979464106733', 'AUC C vs rest': '0.758906343300721', 'AUC D vs rest': '0.876593162609928', 'Accuracy': '0.4997696913864578'})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Register model\n",
    "best_run.register_model(\n",
    "    model_path='outputs/lg-model.pkl', model_name='segmentation-lg',\n",
    "    tags={'Training context':'Hyperdrive'},\n",
    "    properties={\n",
    "        'AUC A vs rest': best_run_metrics['AUC A vs rest'],\n",
    "        'AUC B vs rest': best_run_metrics['AUC B vs rest'],\n",
    "        'AUC C vs rest': best_run_metrics['AUC C vs rest'],\n",
    "        'AUC D vs rest': best_run_metrics['AUC D vs rest'],\n",
    "        'Accuracy': best_run_metrics['Accuracy']\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all child runs, sorted by the primary metric\n",
    "for child_run in run_rf.get_children_sorted_by_primary_metric():\n",
    "    print(child_run)\n",
    "\n",
    "# Get the best run, and its metrics and arguments\n",
    "best_run = run_rf.get_best_run_by_primary_metric()\n",
    "best_run_metrics = best_run.get_metrics()\n",
    "script_arguments = best_run.get_details() ['runDefinition']['arguments']\n",
    "print('Best Run Id: ', best_run.id)\n",
    "print(' -AUC A vs rest:', best_run_metrics['AUC A vs rest'])\n",
    "print(' -AUC B vs rest:', best_run_metrics['AUC B vs rest'])\n",
    "print(' -AUC C vs rest:', best_run_metrics['AUC C vs rest'])\n",
    "print(' -AUC D vs rest:', best_run_metrics['AUC D vs rest'])\n",
    "print(' -Accuracy:', best_run_metrics['Accuracy'])\n",
    "print(' -Arguments:',script_arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register model\n",
    "best_run.register_model(\n",
    "    model_path='outputs/rf-model.pkl', model_name='segmentation-rf',\n",
    "    tags={'Training context':'Hyperdrive'},\n",
    "    properties={\n",
    "        'AUC A vs rest': best_run_metrics['AUC A vs rest'],\n",
    "        'AUC B vs rest': best_run_metrics['AUC B vs rest'],\n",
    "        'AUC C vs rest': best_run_metrics['AUC C vs rest'],\n",
    "        'AUC D vs rest': best_run_metrics['AUC D vs rest'],\n",
    "        'Accuracy': best_run_metrics['Accuracy']\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List registered models\n",
    "for model in Model.list(ws):\n",
    "    print(model.name, 'version:', model.version)\n",
    "    for tag_name in model.tags:\n",
    "        tag = model.tags[tag_name]\n",
    "        print ('\\t',tag_name, ':', tag)\n",
    "    for prop_name in model.properties:\n",
    "        prop = model.properties[prop_name]\n",
    "        print ('\\t',prop_name, ':', prop)\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
